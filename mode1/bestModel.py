import pandas as pd
import numpy as np
import os
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler
import time

# å¯¼å…¥å„ç§æ¨¡å‹
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.neural_network import MLPRegressor
import xgboost as xgb
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF

# å¯è§†åŒ–åº“
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.patches import Rectangle
import matplotlib.patches as patches

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œæ ·å¼
plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei', 'DejaVu Sans',
                                   'WenQuanYi Zen Hei', 'STHeiti', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['font.family'] = 'sans-serif'

plt.style.use('default')  # ä½¿ç”¨é»˜è®¤æ ·å¼
sns.set_palette("husl")

# å¿½ç•¥è­¦å‘Š
import warnings
warnings.filterwarnings('ignore')

print("å½“å‰å·¥ä½œç›®å½•ï¼š", os.getcwd())

# 1. æ•°æ®è¯»å–å’Œé¢„å¤„ç†
print("æ­£åœ¨è¯»å–æ•°æ®...")
df = pd.read_csv("newmerged51_transformed.csv", encoding="utf-8")  # ä¿®æ”¹ä¸ºæ‚¨çš„æ–°æ–‡ä»¶å

# æ£€æŸ¥æ˜¯å¦æœ‰ Unnamed åˆ—å¹¶åˆ é™¤
unnamed_cols = [col for col in df.columns if 'Unnamed' in str(col)]
if unnamed_cols:
    print(f"å‘ç°å¹¶åˆ é™¤ Unnamed åˆ—: {unnamed_cols}")
    df = df.drop(columns=unnamed_cols)

# é‡å‘½ååˆ—åä¸ºæ ‡å‡†æ ¼å¼ï¼ˆå‡è®¾ç¬¬ä¸€åˆ—æ˜¯æ—¶é—´ï¼Œç¬¬äºŒåˆ—æ˜¯èŠ‚ç‚¹ï¼Œç¬¬ä¸‰åˆ—æ˜¯è½¦æµé‡ï¼‰
df.columns = ['time', 'node', 'volume']
print("æ•°æ®åˆ—åå·²é‡å‘½åä¸º: time, node, volume")

print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {df.shape}")
print("æ•°æ®é¢„è§ˆï¼š")
print(df.head())

# 2. æ•°æ®ç±»å‹è½¬æ¢å’Œæ¸…ç†
print("å¼€å§‹æ•°æ®æ¸…ç†...")

# è½¬æ¢æ•°æ®ç±»å‹
df["time"] = pd.to_numeric(df["time"], errors='coerce')
df = df.dropna(subset=['time'])
df["time"] = df["time"].astype(int)

df["node"] = pd.to_numeric(df["node"], errors='coerce')
df = df.dropna(subset=['node'])
df["node"] = df["node"].astype(int)

df["volume"] = pd.to_numeric(df["volume"], errors='coerce')
df = df.dropna(subset=['volume'])

print("æ•°æ®ç±»å‹è½¬æ¢å®Œæˆ")
print(f"æ¸…ç†åæ•°æ®å½¢çŠ¶: {df.shape}")

# 3. å¼‚å¸¸å€¼å¤„ç†
print("å¤„ç†å¼‚å¸¸å€¼...")
Q1 = df['volume'].quantile(0.25)
Q3 = df['volume'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = df[(df['volume'] < lower_bound) | (df['volume'] > upper_bound)]
print(f"å¼‚å¸¸å€¼æ•°é‡: {len(outliers)} ({len(outliers) / len(df) * 100:.2f}%)")

# ç§»é™¤å¼‚å¸¸å€¼
df_clean = df[(df['volume'] >= lower_bound) & (df['volume'] <= upper_bound)]
print(f"æ¸…ç†åæ•°æ®å½¢çŠ¶: {df_clean.shape}")

# 4. ç‰¹å¾å·¥ç¨‹
print("åˆ›å»ºæ–°ç‰¹å¾...")
df_clean = df_clean.copy()

# æ—¶é—´ç‰¹å¾
df_clean['hour'] = (df_clean['time'] % 24).astype('int64')
df_clean['day'] = ((df_clean['time'] // 24) % 30).astype('int64')
df_clean['day_of_week'] = ((df_clean['time'] // 24) % 7).astype('int64')
df_clean['is_weekend'] = df_clean['day_of_week'].isin([5, 6]).astype('int64')

# åˆ¤æ–­é«˜å³°æ—¶æ®µ
df_clean['is_morning_peak'] = df_clean['hour'].between(7, 9).astype('int64')
df_clean['is_evening_peak'] = df_clean['hour'].between(17, 19).astype('int64')
df_clean['is_night'] = df_clean['hour'].between(22, 6).astype('int64')

# èŠ‚ç‚¹ç»Ÿè®¡ç‰¹å¾
node_stats = df_clean.groupby('node')['volume'].agg(['mean', 'std']).reset_index()
node_stats.columns = ['node', 'node_avg_volume', 'node_std_volume']
node_stats['node'] = node_stats['node'].astype('int64')
df_clean = df_clean.merge(node_stats, on='node', how='left')

# æ—¶é—´ç»Ÿè®¡ç‰¹å¾
time_stats = df_clean.groupby('hour')['volume'].agg(['mean', 'std']).reset_index()
time_stats.columns = ['hour', 'hour_avg_volume', 'hour_std_volume']
time_stats['hour'] = time_stats['hour'].astype('int64')
df_clean = df_clean.merge(time_stats, on='hour', how='left')

print("ç‰¹å¾å·¥ç¨‹å®Œæˆ")

# 5. å‡†å¤‡è®­ç»ƒæ•°æ®
feature_cols = ['node', 'time', 'hour', 'day', 'day_of_week', 'is_weekend',
                'is_morning_peak', 'is_evening_peak', 'is_night',
                'node_avg_volume', 'node_std_volume', 'hour_avg_volume', 'hour_std_volume']

X = df_clean[feature_cols]
y = df_clean['volume']

# ä¸ºäº†æé«˜è¿è¡Œé€Ÿåº¦ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªè¾ƒå°çš„æ ·æœ¬è¿›è¡Œæ¨¡å‹å¯¹æ¯”
print("é‡‡æ ·æ•°æ®ä»¥æé«˜è¿è¡Œé€Ÿåº¦...")
if len(X) > 50000:
    sample_size = 50000
    sample_indices = np.random.choice(len(X), sample_size, replace=False)
    X_sample = X.iloc[sample_indices]
    y_sample = y.iloc[sample_indices]
else:
    X_sample = X
    y_sample = y

print(f"ä½¿ç”¨æ ·æœ¬å¤§å°: {len(X_sample)}")

# ç‰¹å¾æ ‡å‡†åŒ–
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_sample)

# æ•°æ®åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y_sample, test_size=0.2, random_state=42
)

print(f"è®­ç»ƒé›†å¤§å°: {X_train.shape}")
print(f"æµ‹è¯•é›†å¤§å°: {X_test.shape}")

# 6. å®šä¹‰æ‰€æœ‰æ¨¡å‹
models = {
    # é›†æˆæ–¹æ³•
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),
    'Extra Trees': ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
    'AdaBoost': AdaBoostRegressor(n_estimators=100, random_state=42),
    'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1),

    # çº¿æ€§æ¨¡å‹
    'Linear Regression': LinearRegression(),
    'Ridge': Ridge(alpha=1.0),
    'Lasso': Lasso(alpha=1.0),
    'ElasticNet': ElasticNet(alpha=1.0, l1_ratio=0.5),

    # æ”¯æŒå‘é‡æœº
    'SVR (RBF)': SVR(kernel='rbf', C=1.0),
    'SVR (Linear)': SVR(kernel='linear', C=1.0),

    # åŸºäºé‚»å±…çš„æ–¹æ³•
    'K-Neighbors': KNeighborsRegressor(n_neighbors=5, n_jobs=-1),

    # å†³ç­–æ ‘
    'Decision Tree': DecisionTreeRegressor(random_state=42),

    # ç¥ç»ç½‘ç»œ
    'Neural Network': MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42),
}

# 7. æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°
results = {}
training_times = {}
predictions = {}

print("\nå¼€å§‹æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°...")
print("=" * 80)

for name, model in models.items():
    print(f"æ­£åœ¨è®­ç»ƒ: {name}")

    try:
        # è®¡ç®—è®­ç»ƒæ—¶é—´
        start_time = time.time()
        model.fit(X_train, y_train)
        training_time = time.time() - start_time
        training_times[name] = training_time

        # é¢„æµ‹
        y_pred = model.predict(X_test)
        predictions[name] = y_pred

        # è¯„ä¼°æŒ‡æ ‡
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        relative_error = rmse / y_test.mean() * 100

        results[name] = {
            'RMSE': rmse,
            'MAE': mae,
            'R2': r2,
            'Relative_Error': relative_error,
            'Training_Time': training_time
        }

        print(f"  âœ“ å®Œæˆ - RMSE: {rmse:.2f}, RÂ²: {r2:.3f}")

    except Exception as e:
        print(f"  âœ— å¤±è´¥: {str(e)}")
        continue

print("\n" + "=" * 80)

# 8. ç»“æœæ±‡æ€»å’Œæ’åº
print("\næ¨¡å‹æ€§èƒ½å¯¹æ¯”ç»“æœ:")
print("=" * 100)
print(f"{'æ¨¡å‹åç§°':<20} {'RMSE':<10} {'MAE':<10} {'RÂ²':<10} {'ç›¸å¯¹è¯¯å·®%':<12} {'è®­ç»ƒæ—¶é—´(s)':<12}")
print("-" * 100)

# æŒ‰RMSEæ’åº
sorted_results = sorted(results.items(), key=lambda x: x[1]['RMSE'])

for name, metrics in sorted_results:
    print(f"{name:<20} {metrics['RMSE']:<10.2f} {metrics['MAE']:<10.2f} "
          f"{metrics['R2']:<10.3f} {metrics['Relative_Error']:<12.2f} "
          f"{metrics['Training_Time']:<12.2f}")

# 9. æœ€ä½³æ¨¡å‹åˆ†æ
best_model_name = sorted_results[0][0]
best_metrics = sorted_results[0][1]

print(f"\næœ€ä½³æ¨¡å‹: {best_model_name}")
print(f"æœ€ä½³RMSE: {best_metrics['RMSE']:.2f}")
print(f"æœ€ä½³ç›¸å¯¹è¯¯å·®: {best_metrics['Relative_Error']:.2f}%")
print(f"æœ€ä½³RÂ²: {best_metrics['R2']:.3f}")

# 10. æ€§èƒ½åˆ†æ
print(f"\næ€§èƒ½åˆ†æ:")
print("-" * 50)

# æ‰¾å‡ºæœ€å¿«çš„æ¨¡å‹
fastest_model = min(training_times.items(), key=lambda x: x[1])
print(f"æœ€å¿«è®­ç»ƒæ¨¡å‹: {fastest_model[0]} ({fastest_model[1]:.2f}s)")

# æ‰¾å‡ºæœ€é«˜å‡†ç¡®åº¦çš„æ¨¡å‹
highest_r2 = max(results.items(), key=lambda x: x[1]['R2'])
print(f"æœ€é«˜RÂ²æ¨¡å‹: {highest_r2[0]} (RÂ²={highest_r2[1]['R2']:.3f})")

# æ‰¾å‡ºæœ€ä½ç›¸å¯¹è¯¯å·®çš„æ¨¡å‹
lowest_relative_error = min(results.items(), key=lambda x: x[1]['Relative_Error'])
print(f"æœ€ä½ç›¸å¯¹è¯¯å·®æ¨¡å‹: {lowest_relative_error[0]} ({lowest_relative_error[1]['Relative_Error']:.2f}%)")

# 11. æ¨èå»ºè®®
print(f"\næ¨èå»ºè®®:")
print("-" * 50)

if best_metrics['Relative_Error'] < 20:
    print("âœ“ æ¨¡å‹æ€§èƒ½è‰¯å¥½ (ç›¸å¯¹è¯¯å·® < 20%)")
elif best_metrics['Relative_Error'] < 30:
    print("âš  æ¨¡å‹æ€§èƒ½ä¸­ç­‰ (ç›¸å¯¹è¯¯å·® 20-30%)")
else:
    print("âœ— æ¨¡å‹æ€§èƒ½è¾ƒå·® (ç›¸å¯¹è¯¯å·® > 30%)")

print(f"\næ ¹æ®ä¸åŒéœ€æ±‚çš„æ¨è:")
print(f"â€¢ è¿½æ±‚æœ€é«˜ç²¾åº¦: {best_model_name}")
print(f"â€¢ è¿½æ±‚è®­ç»ƒé€Ÿåº¦: {fastest_model[0]}")
print(f"â€¢ å¹³è¡¡æ€§èƒ½å’Œé€Ÿåº¦: {sorted_results[1][0] if len(sorted_results) > 1 else best_model_name}")

# 12. ä¿å­˜æœ€ä½³æ¨¡å‹
if not os.path.exists("models"):
    os.makedirs("models")

best_model = models[best_model_name]
import joblib

joblib.dump(best_model, "models/best_model.pkl")
joblib.dump(scaler, "models/scaler.pkl")

print(f"\næœ€ä½³æ¨¡å‹ ({best_model_name}) å·²ä¿å­˜åˆ° models/best_model.pkl")
print("æ ‡å‡†åŒ–å™¨å·²ä¿å­˜åˆ° models/scaler.pkl")

# 13. ç‰¹å¾é‡è¦æ€§åˆ†æï¼ˆå¦‚æœæ”¯æŒï¼‰
if hasattr(best_model, 'feature_importances_'):
    print(f"\n{best_model_name} ç‰¹å¾é‡è¦æ€§:")
    print("-" * 40)
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': best_model.feature_importances_
    }).sort_values('importance', ascending=False)

    for idx, row in feature_importance.head(10).iterrows():
        print(f"{row['feature']:<20}: {row['importance']:.4f}")

# ================================
# 14. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
# ================================

print("\nå¼€å§‹ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")

# åˆ›å»ºç»“æœDataFrameä¾¿äºå¯è§†åŒ–
results_df = pd.DataFrame(results).T
results_df.reset_index(inplace=True)
results_df.rename(columns={'index': 'Model'}, inplace=True)

# åˆ›å»ºå›¾è¡¨ä¿å­˜ç›®å½•
if not os.path.exists("charts"):
    os.makedirs("charts")

# è®¾ç½®å›¾è¡¨æ ·å¼
plt.style.use('default')
colors = plt.cm.Set3(np.linspace(0, 1, len(results_df)))

# å›¾è¡¨1ï¼šæ¨¡å‹æ€§èƒ½å¯¹æ¯”æŸ±çŠ¶å›¾
fig, axes = plt.subplots(2, 2, figsize=(16, 12))
fig.suptitle('æ¨¡å‹æ€§èƒ½å¯¹æ¯”åˆ†æ', fontsize=16, fontweight='bold')

# RMSEå¯¹æ¯”
ax1 = axes[0, 0]
bars1 = ax1.bar(range(len(results_df)), results_df['RMSE'], color=colors)
ax1.set_xlabel('æ¨¡å‹')
ax1.set_ylabel('RMSE')
ax1.set_title('RMSE å¯¹æ¯” (è¶Šå°è¶Šå¥½)')
ax1.set_xticks(range(len(results_df)))
ax1.set_xticklabels(results_df['Model'], rotation=45, ha='right')
ax1.grid(axis='y', alpha=0.3)

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for i, bar in enumerate(bars1):
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width() / 2., height,
             f'{height:.1f}', ha='center', va='bottom', fontsize=8)

# RÂ²å¯¹æ¯”
ax2 = axes[0, 1]
bars2 = ax2.bar(range(len(results_df)), results_df['R2'], color=colors)
ax2.set_xlabel('æ¨¡å‹')
ax2.set_ylabel('RÂ²')
ax2.set_title('RÂ² å¯¹æ¯” (è¶Šå¤§è¶Šå¥½)')
ax2.set_xticks(range(len(results_df)))
ax2.set_xticklabels(results_df['Model'], rotation=45, ha='right')
ax2.grid(axis='y', alpha=0.3)

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for i, bar in enumerate(bars2):
    height = bar.get_height()
    ax2.text(bar.get_x() + bar.get_width() / 2., height,
             f'{height:.3f}', ha='center', va='bottom', fontsize=8)

# MAEå¯¹æ¯”
ax3 = axes[1, 0]
bars3 = ax3.bar(range(len(results_df)), results_df['MAE'], color=colors)
ax3.set_xlabel('æ¨¡å‹')
ax3.set_ylabel('MAE')
ax3.set_title('MAE å¯¹æ¯” (è¶Šå°è¶Šå¥½)')
ax3.set_xticks(range(len(results_df)))
ax3.set_xticklabels(results_df['Model'], rotation=45, ha='right')
ax3.grid(axis='y', alpha=0.3)

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for i, bar in enumerate(bars3):
    height = bar.get_height()
    ax3.text(bar.get_x() + bar.get_width() / 2., height,
             f'{height:.1f}', ha='center', va='bottom', fontsize=8)

# è®­ç»ƒæ—¶é—´å¯¹æ¯”
ax4 = axes[1, 1]
bars4 = ax4.bar(range(len(results_df)), results_df['Training_Time'], color=colors)
ax4.set_xlabel('æ¨¡å‹')
ax4.set_ylabel('è®­ç»ƒæ—¶é—´ (ç§’)')
ax4.set_title('è®­ç»ƒæ—¶é—´å¯¹æ¯” (è¶Šå°è¶Šå¥½)')
ax4.set_xticks(range(len(results_df)))
ax4.set_xticklabels(results_df['Model'], rotation=45, ha='right')
ax4.grid(axis='y', alpha=0.3)

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for i, bar in enumerate(bars4):
    height = bar.get_height()
    ax4.text(bar.get_x() + bar.get_width() / 2., height,
             f'{height:.2f}', ha='left', va='center', fontsize=8)

plt.tight_layout()
plt.savefig('charts/model_comparison_bars.png', dpi=300, bbox_inches='tight')
plt.show()

# å›¾è¡¨2ï¼šæ¨¡å‹æ€§èƒ½çƒ­åŠ›å›¾
fig, ax = plt.subplots(figsize=(12, 8))
# æ ‡å‡†åŒ–æ•°æ®ç”¨äºçƒ­åŠ›å›¾
normalized_data = results_df[['RMSE', 'MAE', 'R2', 'Relative_Error', 'Training_Time']].copy()
# å¯¹äºè¶Šå°è¶Šå¥½çš„æŒ‡æ ‡ï¼Œä½¿ç”¨å€’æ•°
normalized_data['RMSE'] = 1 / normalized_data['RMSE']
normalized_data['MAE'] = 1 / normalized_data['MAE']
normalized_data['Relative_Error'] = 1 / normalized_data['Relative_Error']
normalized_data['Training_Time'] = 1 / normalized_data['Training_Time']

# æ ‡å‡†åŒ–åˆ°0-1èŒƒå›´
from sklearn.preprocessing import MinMaxScaler

scaler_viz = MinMaxScaler()
normalized_data_scaled = scaler_viz.fit_transform(normalized_data)

# åˆ›å»ºçƒ­åŠ›å›¾
im = ax.imshow(normalized_data_scaled, cmap='RdYlGn', aspect='auto')
ax.set_xticks(range(len(normalized_data.columns)))
ax.set_xticklabels(['RMSE', 'MAE', 'RÂ²', 'Relative_Error', 'Training_Time'])
ax.set_yticks(range(len(results_df)))
ax.set_yticklabels(results_df['Model'])
ax.set_title('æ¨¡å‹æ€§èƒ½çƒ­åŠ›å›¾ (ç»¿è‰²=æ›´å¥½)', fontsize=14, fontweight='bold')

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for i in range(len(results_df)):
    for j in range(len(normalized_data.columns)):
        text = ax.text(j, i, f'{normalized_data_scaled[i, j]:.2f}',
                       ha="center", va="center", color="black", fontsize=8)

plt.colorbar(im, ax=ax, label='æ€§èƒ½å¾—åˆ† (0-1)')
plt.tight_layout()
plt.savefig('charts/model_performance_heatmap.png', dpi=300, bbox_inches='tight')
plt.show()

# å›¾è¡¨3ï¼šæ€§èƒ½vsè®­ç»ƒæ—¶é—´æ•£ç‚¹å›¾
fig, ax = plt.subplots(figsize=(12, 8))
scatter = ax.scatter(results_df['Training_Time'], results_df['RMSE'],
                     c=results_df['R2'], s=100, alpha=0.7, cmap='viridis')

# æ·»åŠ æ¨¡å‹åç§°æ ‡ç­¾
for i, txt in enumerate(results_df['Model']):
    ax.annotate(txt, (results_df['Training_Time'].iloc[i], results_df['RMSE'].iloc[i]),
                xytext=(5, 5), textcoords='offset points', fontsize=9)

ax.set_xlabel('è®­ç»ƒæ—¶é—´ (ç§’)')
ax.set_ylabel('RMSE')
ax.set_title('æ¨¡å‹æ€§èƒ½ vs è®­ç»ƒæ—¶é—´ (æ°”æ³¡é¢œè‰²ä»£è¡¨RÂ²)')
ax.grid(True, alpha=0.3)
plt.colorbar(scatter, label='RÂ²')
plt.tight_layout()
plt.savefig('charts/performance_vs_time.png', dpi=300, bbox_inches='tight')
plt.show()

print("\n" + "=" * 60)
print("ğŸ“Š åˆ†æå®Œæˆï¼ä¸»è¦å›¾è¡¨å·²ç”Ÿæˆ")
print("=" * 60)
print("ç”Ÿæˆçš„å›¾è¡¨åŒ…æ‹¬ï¼š")
print("1. ğŸ“Š model_comparison_bars.png - æ¨¡å‹æ€§èƒ½å¯¹æ¯”æŸ±çŠ¶å›¾")
print("2. ğŸ”¥ model_performance_heatmap.png - æ¨¡å‹æ€§èƒ½çƒ­åŠ›å›¾")
print("3. ğŸ’« performance_vs_time.png - æ€§èƒ½vsè®­ç»ƒæ—¶é—´æ•£ç‚¹å›¾")
print(f"\næ‰€æœ‰å›¾è¡¨å·²ä¿å­˜åˆ° 'charts/' ç›®å½•")
print("=" * 60)

print(f"\nâœ… ç»¼åˆæ¨¡å‹å¯¹æ¯”åˆ†æå®Œæˆï¼")
print(f"ğŸ“ æ¨¡å‹æ–‡ä»¶: models/best_model.pkl")
print(f"ğŸ“ æ ‡å‡†åŒ–å™¨: models/scaler.pkl")
print(f"ğŸ“ å¯è§†åŒ–å›¾è¡¨: charts/ç›®å½•")